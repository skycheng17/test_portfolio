setwd("~/Documents/GitHub/skycheng17.github.io/portfolio/Flooding")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
rm(list=ls())
library(tidyverse)
library(sf)
library(ggplot2)
library(corrplot)
library(ggcorrplot)
library(readr)
library(reshape2)
library(stargazer)
library(car)
library(dplyr)
library(MASS)
library(grDevices)
library(lubridate)
library(tidycensus)
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
rm(list=ls())
library(tidyverse)
library(sf)
library(ggplot2)
library(corrplot)
library(ggcorrplot)
library(readr)
library(reshape2)
library(stargazer)
library(car)
library(dplyr)
#library(MASS)
library(grDevices)
library(lubridate)
library(tidycensus)
library(tidyverse)
library(sf)
library(ggplot2)
library(corrplot)
library(ggcorrplot)
library(readr)
library(reshape2)
library(stargazer)
library(car)
library(dplyr)
#library(MASS)
library(grDevices)
library(lubridate)
library(tidycensus)
# Load National Risk Index
nri_county <- read.csv("./data/NRI_Table_Counties.csv", header = TRUE)
# Cleaning to only the necessary columns
nri_county_clean <- nri_county[, c("STATE", "STATEFIPS", "COUNTY", "COUNTYTYPE", "COUNTYFIPS", "NRI_ID", "POPULATION", "BUILDVALUE", "AREA", "RISK_VALUE", "RISK_SCORE", "RISK_RATNG", "EAL_VALT", "EAL_VALB", "EAL_VALP", "EAL_VALPE", "EAL_VALA", "CFLD_EVNTS", "CFLD_AFREQ", "CFLD_EXPB", "CFLD_EXPP", "CFLD_EXPPE", "CFLD_EXPT", "CFLD_EXP_AREA", "CFLD_HLRB", "CFLD_HLRP", "CFLD_HLRR", "CFLD_EALB", "CFLD_EALP", "CFLD_EALPE", "CFLD_EALT", "CFLD_EALS", "CFLD_EALR", "CFLD_ALRB", "CFLD_ALRP", "CFLD_ALR_NPCTL", "CFLD_RISKV", "CFLD_RISKS", "CFLD_RISKR", "RFLD_EVNTS", "RFLD_AFREQ", "RFLD_EXPB", "RFLD_EXPP", "RFLD_EXPPE", "RFLD_EXPA", "RFLD_EXPT", "RFLD_EXP_AREA", "RFLD_HLRB", "RFLD_HLRP", "RFLD_HLRA", "RFLD_HLRR", "RFLD_EALB", "RFLD_EALP", "RFLD_EALPE", "RFLD_EALA", "RFLD_EALT", "RFLD_EALS", "RFLD_EALR", "RFLD_ALRB", "RFLD_ALRP", "RFLD_ALRA", "RFLD_ALR_NPCTL", "RFLD_RISKV", "RFLD_RISKS", "RFLD_RISKR")]
summary(nri_county_clean$CFLD_RISKV)
summary(nri_county_clean$RFLD_RISKV)
#ranking top 50 high-risk, high-density counties
high_flood_risk_counties <- nri_county_clean %>%
filter(CFLD_RISKV >= 1228324 & RFLD_RISKV >= 1228324)
high_flood_risk_counties_sorted <- high_flood_risk_counties %>%
arrange(desc(POPULATION/AREA))
top_50_counties <- high_flood_risk_counties_sorted %>%
slice(1:50)
list(top_50_counties$COUNTY)
#graphs and maps of top 50 counties
# Create a scatter plot of risk value by county
ggplot(top_50_counties, aes(x = COUNTY, y = RISK_VALUE, color = POPULATION)) +
geom_point() +
labs(title = "Risk Scatter Plot by County",
x = "County", y = "Risk Value") +
scale_color_gradient(low = "lightblue", high = "darkblue") +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
# Graph data by risk value
ggplot(top_50_counties, aes(x = reorder(COUNTY, RISK_VALUE), y = RISK_VALUE, fill = RISK_VALUE)) +
geom_bar(stat = "identity", width = 0.7) +
scale_fill_gradient(low = "lightblue", high = "darkblue") +
labs(title = "Top 50 Counties with High Density and Flooding Risk", x = "County", y = "Risk Value") +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 8)) +
theme(axis.text.y = element_text(size = 8)) +
theme(plot.title = element_text(size = 12, hjust = 0.5, face = "bold"))
write.csv(top_50_counties, file = "top_50_counties_1.csv", row.names = FALSE)
#To visualize change in home value over time for the top 50 counties, the top 50 counties datafram was merged with ZHVI data in excel. The following dataframe shows this information.
top_50_counties_housing<-read.csv("./data/top_50_counties_housing.csv")
# Pick the columns relevant
zillowcounty50 <- top_50_counties_housing[, c(3, 182:325)]
# Reshape data into long format
colnames(zillowcounty50) <- sub("^X", "", colnames(zillowcounty50))
zillowcounty50long <- zillowcounty50 %>%
gather(key = "date", value = "typical_home_value", -COUNTY)
# Convert date column to proper Date format
zillowcounty50long <- zillowcounty50long %>%
mutate(date = as.Date(date, format = "%d.%m.%Y"))
#riskzillowcountylong_clean <- na.omit(riskzillowcountylong)
# Plot the top 50 counties's typical home value over the 2010-2021 time period
ggplot(data = zillowcounty50long, aes(x = date, y = typical_home_value, color = COUNTY)) +
geom_line(size = 1, alpha = 0.8, linetype = "solid") +
labs(title = "Typical Home Value by County over Time",
x = "Year",
y = "Typical Home Value") +
scale_color_manual(values = colorRampPalette(colors = c("blue", "pink","purple", "grey"))(50)) +
theme(plot.title = element_text(size = 18, face = "bold"),
axis.title = element_text(size = 12),
axis.text = element_text(size = 8),
legend.text = element_text(size = 5), # decrease font size of legend text
legend.title = element_text(size = 5)) +
scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
expand_limits(y = 0) +
theme(legend.key.width = unit(0.3, "cm")) +
coord_cartesian(ylim = c(0, 1500000))
nri_county_extraclean<-read.csv("./data/nri_county_clean.csv")
zillowdata<-read.csv("./data/County_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv")
# Trim whitespace
nri_county_extraclean$COUNTY <- trimws(nri_county_extraclean$COUNTY)
zillowdata$COUNTY <- trimws(zillowdata$COUNTY)
unique(nri_county_extraclean$COUNTY)
unique(zillowdata$COUNTY)
# Merge the datasets
merged_df <- merge(nri_county_extraclean, zillowdata[, c(3, 8, 130:273)], by = c("COUNTY", "STATEFIPS"), all = TRUE)
#Create new variable to look at the change in housing value over the ten year period
merged_df$HOUSING_VALUE_CHANGE <- merged_df$X31.12.2021 - merged_df$X31.10.2010
#Create a new variable that is a coefficient of variation for each county's housing price data (CV)
# calculate the CV for each county (higher the CV, the more fluctuation)
merged_df$CV <- apply(merged_df[,66:209], 1, function(x) {sd(x) / mean(x)})
# view the dataframe by the 'CV' column to see the most and least fluctuating counties
merged_df[order(merged_df$CV, decreasing = TRUE), ]
write.csv(merged_df, file = "merged_df.csv", row.names = FALSE)
#dataset cleaned on excel to ease merging (correcting naming)
dp04 <- read.csv("./data/DP04Data.csv", header = TRUE)
dp04_2010 <- read.csv("./data/2010_DP04Data.csv", header = TRUE)
dp04_2010 <- dp04_2010 %>% rename(GEOID = GEO_ID)
dp05 <- read.csv("./data/2021_demographics.csv", header = TRUE)
dp05_2010 <- read.csv("./data/2010_demographics.csv", header = TRUE)
merged_df <- read.csv("./data/merged_df.csv")
merged_df_clean <- read.csv("./data/merged_df_cleaned.csv")
# Merge dp04 and dp04_2010 through the GEOID column
merged_dp04 <- merge(dp04, dp04_2010, by = "GEOID", suffixes = c("_2021", "_2010"))
merged_dp04 <- merged_dp04%>% rename(COUNTY = COUNTY_2021)
merged_dp04 <- merged_dp04%>% rename(STATE = STATE_2021)
# Merge dp05 and dp05_2010 through the GEOID column
merged_dp05 <- merge(dp05, dp05_2010, by = "GEOID", suffixes = c("_2021", "_2010"))
merged_dp05 <- merged_dp05%>% rename(COUNTY = COUNTY_2021)
merged_dp05 <- merged_dp05%>% rename(STATE = STATE_2021)
# Create a GEOID column based on the NRI_ID by correcting the string and pasting the write code to create a new correct GEOID column
merged_df_clean$NRI_ID_CLEAN <- substr(merged_df_clean$NRI_ID, 2, 7)
merged_df_clean$GEOID <- paste0("0500000US", merged_df_clean$NRI_ID_CLEAN)
#Trim whitespace to merge
merged_df_clean$GEOID <- trimws(merged_df_clean$GEOID)
merged_dp04$GEOID <- trimws(merged_dp04$GEOID)
merged_dp05$GEOID <- trimws(merged_dp05$GEOID)
unique(merged_df_clean$GEOID)
unique(merged_dp04$GEOID)
unique(merged_dp05$GEOID)
merged_df1 <- merge(merged_df_clean, merged_dp04[, c(1, 5, 6:108)], by = ("GEOID"), all = TRUE)
merged_df1 <- merge(merged_df1, merged_dp05[, c(1, 5:45)], by = ("GEOID"), all = TRUE)
# View the merged data
head(merged_df1)
write.csv(merged_df1, file = "merged_df1.csv", row.names = FALSE)
# Calculate adjusted 2010 housing value
merged_df1[, 15:26][merged_df1[, 15:26] == "#DIV/0!"] <- NA
merged_df1$X2010_housing <- as.numeric(merged_df1$X2010_housing)
merged_df1$adjusted_housing_2010 <- merged_df1$X2010_housing * (1 + 0.3842)
# Calculate difference between adjusted 2010 housing value and 2021 housing value
merged_df1$zhvi2010_2021 <- merged_df1$X2021_housing - merged_df1$adjusted_housing_2010
#names(merged_df1) <- gsub("\\.y$", "", names(merged_df1))
# Calculate summary statistics for zhvi2010_2021
summary(merged_df1$zhvi2010_2021)
#Creating Models
mod1 <- lm(log(zhvi2010_2021) ~ log(RISK_VALUE), data = merged_df1)
summary(mod1)
cor.test(log(merged_df1$zhvi2010_2021), log(merged_df1$RISK_VALUE), method = "pearson")
mod2 <- lm(log(zhvi2010_2021) ~ log(EAL_VALT), data = merged_df1)
summary(mod2)
cor.test(log(merged_df1$zhvi2010_2021), log(merged_df1$EAL_VALT), method = "pearson")
mod3 <- lm(log(zhvi2010_2021) ~ (RFLD_EVNTS), data = merged_df1)
summary(mod3)
cor.test(log(merged_df1$zhvi2010_2021), (merged_df1$RFLD_EVNTS), method = "pearson")
mod4 <- lm(log(zhvi2010_2021) ~ (RFLD_EALB), data = merged_df1)
summary(mod4)
cor.test(log(merged_df1$zhvi2010_2021), (merged_df1$RFLD_EALB), method = "pearson")
mod5 <- lm(log(zhvi2010_2021) ~ (CFLD_RISKS), data = merged_df1)
summary(mod5)
cor.test(log(merged_df1$zhvi2010_2021), (merged_df1$CFLD_RISKS), method = "pearson")
mod6 <- lm(log(zhvi2010_2021) ~ (RFLD_EXP_AREA), data = merged_df1)
summary(mod6)
cor.test(log(merged_df1$zhvi2010_2021), (merged_df1$RFLD_EXP_AREA), method = "pearson")
mod7 <- lm(log(zhvi2010_2021) ~ (RFLD_EALT), data = merged_df1)
summary(mod7)
cor.test(log(merged_df1$zhvi2010_2021), (merged_df1$RFLD_EALT), method = "pearson")
mod8 <- lm(log(zhvi2010_2021) ~ (RFLD_RISKS), data = merged_df1)
summary(mod8)
cor.test(log(merged_df1$zhvi2010_2021), (merged_df1$RFLD_RISKS), method = "pearson")
# Plot
ggplot(merged_df1, aes(x = log(RISK_VALUE), y = log(zhvi2010_2021))) +
geom_point(na.rm = TRUE) +
labs(x = "Log Risk Value", y = "Log Housing Value Change") +
geom_smooth(method = "lm", se = FALSE)
ggplot(merged_df1, aes(x = log(EAL_VALT), y = log(zhvi2010_2021))) +
geom_point(na.rm = TRUE) +
labs(x = "Log Expected Annual Loss - Total Composite", y = "Log Housing Value Change") +
geom_smooth(method = "lm", se = FALSE)
ggplot(merged_df1, aes(x = (RFLD_EVNTS), y = log(zhvi2010_2021))) +
geom_point(na.rm = TRUE) +
labs(x = "Log Riverine Flooding Events", y = "Log Housing Value Change") +
geom_smooth(method = "lm", se = FALSE)
ggplot(merged_df1, aes(x = log(RFLD_EALB), y = log(zhvi2010_2021))) +
geom_point(na.rm = TRUE) +
labs(x = "Log Riverine Flooding - Expected Annual Loss to Buildings", y = "Log Housing Value Change") +
geom_smooth(method = "lm", se = FALSE)
ggplot(merged_df1, aes(x = log(CFLD_RISKS), y = log(zhvi2010_2021))) +
geom_point(na.rm = TRUE) +
labs(x = "Log Coastal Flooding - Hazard Type Risk Index Score", y = "Log Housing Value Change") +
geom_smooth(method = "lm", se = FALSE)
ggplot(merged_df1, aes(x = log(RFLD_EXP_AREA), y = log(zhvi2010_2021))) +
geom_point(na.rm = TRUE) +
labs(x = "Log Riverine Flooding - Expected Area", y = "Log Housing Value Change") +
geom_smooth(method = "lm", se = FALSE)
ggplot(merged_df1, aes(x = log(RFLD_EALT), y = log(zhvi2010_2021))) +
geom_point(na.rm = TRUE) +
labs(x = "Log Riverine Flooding - Expected Annual Loss - Total", y = "Log Housing Value Change") +
geom_smooth(method = "lm", se = FALSE)
ggplot(merged_df1, aes(x = log(RFLD_RISKS), y = log(zhvi2010_2021))) +
geom_point(na.rm = TRUE) +
labs(x = "Log Riverine Flooding - Hazard Type Risk Index Score", y = "Log Housing Value Change") +
geom_smooth(method = "lm", se = FALSE)
# extract coefficients and p-values from each model
library(broom)
library(knitr)
tidy_data <- rbind(
tidy(mod1),
tidy(mod2),
tidy(mod3),
tidy(mod4),
tidy(mod5),
tidy(mod6),
tidy(mod7),
tidy(mod8)
)
# format table using knitr::kable()
kable(tidy_data, digits = c(2, 2, 2, 2), align = "c")
library(dplyr)
cordat <- merged_df1[, c("RISK_VALUE", "EAL_VALT", "RFLD_EVNTS", "RFLD_EALB", "CFLD_RISKS", "RFLD_EXP_AREA", "RFLD_EALT", "RFLD_RISKS", "zhvi2010_2021")]
cordat <- na.omit(cordat)
corr <- round(cor(cordat), 2)
ggcorrplot(corr, hc.order = TRUE, type = "lower",
lab = TRUE, )
# Fit a full multiple regression model with all predictors
#model_full <- lm( HOUSING_VALUE_CHANGE ~RISK_VALUE+EAL_VALT+CFLD_AFREQ+CFLD_EALT+CFLD_RISKS+RFLD_AFREQ+RFLD_EALT+RFLD_RISKS+HOUSING_VALUE_CHANGE, data = merged_df)
model_full <- lm( HOUSING_VALUE_CHANGE ~ POPULATION+BUILDVALUE+AREA+RISK_VALUE+EAL_VALT+EAL_VALB+EAL_VALP+EAL_VALPE+EAL_VALA+CFLD_RISKS+RFLD_EVNTS+RFLD_AFREQ+RFLD_EXPB+RFLD_EXPP+RFLD_EXPPE+RFLD_EXPA+RFLD_EXPT+RFLD_EXP_AREA+RFLD_HLRB+RFLD_HLRP+RFLD_HLRA+RFLD_EALB+RFLD_EALP+RFLD_EALPE+RFLD_EALA+RFLD_EALT+RFLD_EALS++RFLD_ALRB+RFLD_ALRP+RFLD_ALRA+RFLD_ALR_NPCTL+RFLD_RISKV+RFLD_RISKS, data = merged_df)
# Use stepAIC to perform backwards selection
model_backwards <- stepAIC(model_full, direction = "backward")
model_backwards
# Check for multicollinearity
vif(model_backwards)
merged_df2 <- read.csv("./data/merged_df1_anu.csv", header = TRUE)
#Creating new variables
#Percent White and change from 2010 to 2021
merged_df2$PERCWHITE2021 <- merged_df2$white / merged_df2$tot_pop
merged_df2$PERCWHITE2010 <- merged_df2$X2010_white / merged_df2$X2010_tot_pop
merged_df2$PWCHANGE <- merged_df2$PERCWHITE2021 - merged_df2$PERCWHITE2010
#Percent Male and change from 2010 to 2021
merged_df2$PERCMALE2021 <- merged_df2$male / merged_df2$tot_pop
merged_df2$PERCMALE2010 <- merged_df2$X2010_male / merged_df2$X2010_tot_pop
merged_df2$PMCHANGE <- merged_df2$PERCMALE2021 - merged_df2$PERCMALE2010
#Change in median age from 2010 to 2021
merged_df2$MEDAGECHANGE <- merged_df2$median_age - merged_df2$X2010_median_age
sum(is.na(merged_df2$MEDAGECHANGE))
merged_df2 <- merged_df2[!is.na(merged_df1$MEDAGECHANGE), ]
#Black to white population ratio and change from 2010 to 2021
merged_df2$BTOW2021 <- merged_df2$black / merged_df2$white
merged_df2$BTOW2010 <- merged_df2$X2010_black / merged_df2$X2010_white
merged_df2$BTOWCHANGE <- merged_df2$BTOW2021 - merged_df2$BTOW2010
sum(is.na(merged_df2$PWCHANGE))
sum(is.na(merged_df2$RISK_VALUE))
sum(is.na(merged_df2$EAL_VALT))
sum(is.na(merged_df2$CFLD_EALT))
sum(is.na(merged_df2$RFLD_EALT))
#New demographic models
mod10 <- (lm(PWCHANGE ~ CFLD_RISKS + RFLD_EVNTS + RFLD_EXPA + RFLD_EXP_AREA
+ RFLD_HLRA + RFLD_EALB + RFLD_EALA + RFLD_ALR_NPCTL + RFLD_RISKS, data = merged_df1))
mod10 <- (lm(PWCHANGE ~ CFLD_RISKS + RFLD_EVNTS + RFLD_EXPA + RFLD_EXP_AREA
+ RFLD_HLRA + RFLD_EALB + RFLD_EALA + RFLD_ALR_NPCTL + RFLD_RISKS, data = merged_df1))
merged_df2$PWCHANGE <- merged_df2$PERCWHITE2021 - merged_df2$PERCWHITE2010
mod10 <- (lm(PWCHANGE ~ CFLD_RISKS + RFLD_EVNTS + RFLD_EXPA + RFLD_EXP_AREA
+ RFLD_HLRA + RFLD_EALB + RFLD_EALA + RFLD_ALR_NPCTL + RFLD_RISKS, data = merged_df1))
merged_df2 <- read.csv("./data/merged_df1_anu.csv", header = TRUE)
#Creating new variables
#Percent White and change from 2010 to 2021
merged_df2$PERCWHITE2021 <- merged_df2$white / merged_df2$tot_pop
merged_df2$PERCWHITE2010 <- merged_df2$X2010_white / merged_df2$X2010_tot_pop
merged_df2$PWCHANGE <- merged_df2$PERCWHITE2021 - merged_df2$PERCWHITE2010
#Percent Male and change from 2010 to 2021
merged_df2$PERCMALE2021 <- merged_df2$male / merged_df2$tot_pop
merged_df2$PERCMALE2010 <- merged_df2$X2010_male / merged_df2$X2010_tot_pop
merged_df2$PMCHANGE <- merged_df2$PERCMALE2021 - merged_df2$PERCMALE2010
#Change in median age from 2010 to 2021
merged_df2$MEDAGECHANGE <- merged_df2$median_age - merged_df2$X2010_median_age
sum(is.na(merged_df2$MEDAGECHANGE))
merged_df2 <- merged_df2[!is.na(merged_df1$MEDAGECHANGE), ]
#Black to white population ratio and change from 2010 to 2021
merged_df2$BTOW2021 <- merged_df2$black / merged_df2$white
merged_df2$BTOW2010 <- merged_df2$X2010_black / merged_df2$X2010_white
merged_df2$BTOWCHANGE <- merged_df2$BTOW2021 - merged_df2$BTOW2010
sum(is.na(merged_df2$PWCHANGE))
sum(is.na(merged_df2$RISK_VALUE))
sum(is.na(merged_df2$EAL_VALT))
sum(is.na(merged_df2$CFLD_EALT))
sum(is.na(merged_df2$RFLD_EALT))
#New demographic models
mod10 <- (lm(PWCHANGE ~ CFLD_RISKS + RFLD_EVNTS + RFLD_EXPA + RFLD_EXP_AREA
+ RFLD_HLRA + RFLD_EALB + RFLD_EALA + RFLD_ALR_NPCTL + RFLD_RISKS, data = merged_df2))
View(merged_df2)
merged_df2 <- read.csv("./data/merged_df1_anu.csv", header = TRUE)
View(merged_df1)
View(merged_df2)
merged_df2$PERCWHITE2021 <- merged_df2$white / merged_df2$tot_pop
merged_df2$PERCWHITE2010 <- merged_df2$X2010_white / merged_df2$X2010_tot_pop
merged_df2$PWCHANGE <- merged_df2$PERCWHITE2021 - merged_df2$PERCWHITE2010
#Percent Male and change from 2010 to 2021
merged_df2$PERCMALE2021 <- merged_df2$male / merged_df2$tot_pop
merged_df2$PERCMALE2010 <- merged_df2$X2010_male / merged_df2$X2010_tot_pop
merged_df2$PMCHANGE <- merged_df2$PERCMALE2021 - merged_df2$PERCMALE2010
#Change in median age from 2010 to 2021
merged_df2$MEDAGECHANGE <- merged_df2$median_age - merged_df2$X2010_median_age
sum(is.na(merged_df2$MEDAGECHANGE))
merged_df2 <- merged_df2[!is.na(merged_df1$MEDAGECHANGE), ]
#Black to white population ratio and change from 2010 to 2021
merged_df2$BTOW2021 <- merged_df2$black / merged_df2$white
merged_df2$BTOW2010 <- merged_df2$X2010_black / merged_df2$X2010_white
merged_df2$BTOWCHANGE <- merged_df2$BTOW2021 - merged_df2$BTOW2010
sum(is.na(merged_df2$PWCHANGE))
sum(is.na(merged_df2$RISK_VALUE))
sum(is.na(merged_df2$EAL_VALT))
sum(is.na(merged_df2$CFLD_EALT))
sum(is.na(merged_df2$RFLD_EALT))
mod10 <- (lm(PWCHANGE ~ CFLD_RISKS + RFLD_EVNTS + RFLD_EXPA + RFLD_EXP_AREA
+ RFLD_HLRA + RFLD_EALB + RFLD_EALA + RFLD_ALR_NPCTL + RFLD_RISKS, data = merged_df2))
